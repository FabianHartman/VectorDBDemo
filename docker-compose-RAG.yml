services:
  ollama-rag:
    image: ollama/ollama:latest
    container_name: ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    runtime: nvidia
    ports:
      - "3304:11434"
    volumes:
      - ollama_rag_data:/root/.ollama
    entrypoint: ["/bin/sh", "-c", "ollama serve & sleep 5 && ollama pull mxbai-embed-large && ollama pull llama3.1 && pkill -f \"ollama serve\" && ollama serve"]

volumes:
  ollama_rag_data: